# 🧠 Custom GPT-2 Model in TensorFlow

This project contains a complete implementation of the GPT-2 architecture from scratch using TensorFlow and Keras. It demonstrates core transformer mechanics and enables you to build a simplified GPT-style language model step by step.

---

## 🔍 Highlights
- ✅ 12 Transformer layers (blocks)
- 🧠 Multi-head Self-Attention (12 heads)
- 🧩 Token & Positional Embeddings
- 💡 Feedforward networks (3072-dimensional)
- 🔄 Causal (masked) self-attention for autoregressive generation
- 🧮 Total Parameters: ~163 million

- 🔡 Vocabulary Size: 50,257
- 📏 Supports up to 1024-token sequences

---

## 📊 Parameters Snapshot

Here's a screenshot showing the model summary and total parameters:

![Model Parameters](https://github.com/user-attachments/assets/da69117d-fcc0-416b-aaf7-4eb580883e6e)
> Total trainable parameters: **163,087,441**

---


