# ğŸ§  Custom GPT-2 Model in TensorFlow

This project contains a complete implementation of the GPT-2 architecture from scratch using TensorFlow and Keras. It demonstrates core transformer mechanics and enables you to build a simplified GPT-style language model step by step.

---

## ğŸ” Highlights
- âœ… 12 Transformer layers (blocks)
- ğŸ§  Multi-head Self-Attention (12 heads)
- ğŸ§© Token & Positional Embeddings
- ğŸ’¡ Feedforward networks (3072-dimensional)
- ğŸ”„ Causal (masked) self-attention for autoregressive generation
- ğŸ§® Total Parameters: ~163 million

- ğŸ”¡ Vocabulary Size: 50,257
- ğŸ“ Supports up to 1024-token sequences

---

## ğŸ“Š Parameters Snapshot

Here's a screenshot showing the model summary and total parameters:

![Model Parameters](https://github.com/user-attachments/assets/da69117d-fcc0-416b-aaf7-4eb580883e6e)
> Total trainable parameters: **163,087,441**

---


